{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do: \n",
    "* standard data types\n",
    "* clean data\n",
    "** clean corrupted data points\n",
    "** ensure uniform formatting\n",
    "** handle missing values\n",
    "* explore relationships among data (tbd)\n",
    "* feature engineering\n",
    "* do modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.feature_selection as fs\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "Y = df['SalePrice']\n",
    "df = df.drop(['SalePrice'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns\n",
    "df.head().loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration I\n",
    "\n",
    "Upon doing a quick inspection of the data, I've devised with the following feature handling strategy: \n",
    "- variables with less than 20 unique values will be treated as categorical features, with the exception of\n",
    "    - PoolArea\n",
    "- the following features have more than 20 distinct values but will be treated as categorical\n",
    "    - Neighborhood\n",
    "    - YearBuilt\n",
    "    - YearRemodAdd\n",
    "    - GarageYrBlt\n",
    "- the following features have less than 20 unique values, but will more appropriately be treated as continuous features\n",
    "    - BsmtFullBath\n",
    "    - BsmtHalfBath\n",
    "    - FullBath\n",
    "    - HalfBath\n",
    "    - BedroomAbvGr\n",
    "    - KitchenAbvGr\n",
    "    - TotRmsAbvGrd\n",
    "    - Fireplaces\n",
    "    - GarageCars\n",
    "    - MoSold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "not_converted = []\n",
    "for col in df.columns:\n",
    "    if (len(df[col].value_counts()) < 20):\n",
    "        if col != 'PoolArea':\n",
    "            df[col] = df[col].astype('category')\n",
    "#             print(col + ' converted to Category')\n",
    "    else:\n",
    "        not_converted.append(col)\n",
    "        \n",
    "# Categorical variables that have more than 20 distinct values.\n",
    "df['Neighborhood'] = df['Neighborhood'].astype('category')\n",
    "df['YearBuilt'] = df['YearBuilt'].astype('category')\n",
    "df['YearRemodAdd'] = df['YearRemodAdd'].astype('category')\n",
    "df['GarageYrBlt'] = df['GarageYrBlt'].dropna().astype(int).astype('category')\n",
    "\n",
    "# Variables with less than 20 distinct values, that are not categorical. They contain counts, and \n",
    "# will be treated as continuous/numeric.\n",
    "to_convert = ['BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', \n",
    "              'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', \n",
    "              'Fireplaces', 'GarageCars']\n",
    "\n",
    "for col in to_convert:\n",
    "    # convert cols back to int. \n",
    "    df[col] = df[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Separate out columns by data type to fill in missing values\n",
    "integer_columns = df.select_dtypes(include=[np.int64, np.float64])\n",
    "categorical_columns = df.select_dtypes(include=['category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the numerical variables\n",
    "_ = integer_columns.hist(bins=10, figsize=(15, 20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: \n",
    "    - The variables that are approximately normally distributed or are normal with a skew, tend to be related to square footage.\n",
    "    - There are a handful of discrete numerical variables representing counts\n",
    "    - There are about 9 variables that look like they have a very low variability in information aka have a high percentage of one value.\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the categorical variables\n",
    "\n",
    "try:\n",
    "    fig, ax = plt.subplots(11, 4)\n",
    "    for i, cat in enumerate(categorical_columns.columns):\n",
    "        _ = categorical_columns[cat].value_counts().plot('bar', ax=ax[i//4][i % 4], figsize=(15, 80)).set_title(cat);\n",
    "    plt.show();\n",
    "except IndexError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High percentage of NAs\n",
    "\n",
    "Identifying variables with more than some percentage of NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thresh = .8\n",
    "nulls = df.isna().sum()\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.barh(nulls[nulls > df.shape[0]*thresh].sort_values(ascending=False).index, \n",
    "         (nulls[nulls > df.shape[0]*thresh].sort_values(ascending=False)/df.shape[0]*100).values)\n",
    "fig.suptitle('Columns with percentage of missing values greater than 80%', fontsize=15)\n",
    "plt.xlabel('%', fontsize=12)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot suggests we should drop Alley, PoolQC, Fence, MiscFeature because they are over 80% null.\n",
    "Upon looking at the data dict however, you'll see that the NAs represent the case where a house does not have a given feature, and are not actually `nulls`. \n",
    "Let this be a reminder to understand what the data represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate two fields using linear interpolation\n",
    "integer_columns = integer_columns.interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Handling the two fields where the NaNs do not correspond to a valid value.\n",
    "# Fields 'Electrical' and \"MasVnrType\" have 8 and 1 missing values respectively. Since these are two fields\n",
    "# where the NaN does not correspond to a value, and since there are so few records, we're dropping those.\n",
    "categorical_columns = categorical_columns[~categorical_columns['Electrical'].isna()]\n",
    "categorical_columns = categorical_columns[~categorical_columns['MasVnrType'].isna()]\n",
    "\n",
    "# adding 'None' category to categorical fields, and then replacing NaN with 'None' for processing\n",
    "def add_category(df, col):\n",
    "    df[col].cat.add_categories('None', inplace=True)\n",
    "null_categorical = categorical_columns.isna().sum().sort_values(ascending=False).reset_index()\n",
    "for index, row in null_categorical.iterrows():\n",
    "    categorical_columns[row['index']].cat.categories\n",
    "    if row[0] > 0:\n",
    "        add_category(categorical_columns, row['index'])\n",
    "        categorical_columns[row['index']] = categorical_columns[row['index']].fillna(value='None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a model with variables that have a covariance below a certain threshold. When running a covariance matrix on the dataframe, we find that the covariances range from -0.04 to 100,000. Let's set the covariance threshold at 5000 and see what kind of results we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test = df.cov()[abs(df.cov()) < 5000]\n",
    "\n",
    "test['LotFrontage'].loc[~test['LotFrontage'].isnull()].index\n",
    "\n",
    "# for col in test.columns:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying variables with a covariance higher than 10,000\n",
    "high_cov_cols = dict()\n",
    "gt_thresh = df.cov()[abs(df.cov()) > 10000]\n",
    "for col in df.cov():\n",
    "    high_cov_cols[col] = list(df.cov().index[gt_thresh[col].notnull()])\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "high_cov_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['LotArea']\n",
    "Y = df['SalePrice']\n",
    "# X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = sm.OLS(endog=Y, exog=X.astype(float))\n",
    "results = model.fit()\n",
    "predictions = results.predict()\n",
    "\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fields to check correlations between\n",
    "-\n",
    "\n",
    "df.corr()[df.corr() > 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Separate out columns by data type to fill in missing values\n",
    "integer_columns = df.select_dtypes(include=[np.int64, np.float64])\n",
    "categorical_columns = df.select_dtypes(include=['category'])\n",
    "\n",
    "# plt.hist(integer_columns, bins=10)\n",
    "_ = integer_columns.hist(bins=10, figsize=(15, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "categorical_columns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(11, 4)\n",
    "for i, cat in enumerate(categorical_columns.columns):\n",
    "    categorical_columns[cat].value_counts().plot('bar', ax=ax[i//4][i % 4], figsize=(15, 50)).set_title(cat);\n",
    "    \n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_filled : original df with missing values filled in\n",
    "# rebuild df with integer columns and categorical columns, with NAs filled in.\n",
    "# interpolated integer columns with the default settings, no limits on the nature of the interpolation\n",
    "# doing a forward fill and backfill on categorical columns\n",
    "df_filled = pd.concat([integer_columns.interpolate(), categorical_columns.ffill().bfill()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(df_filled.drop(['SalePrice'], axis=1)).dropna()\n",
    "y = df_filled['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.columns[~feat_sel.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting with Feature Selection. Select features with a variance greater than 0.1.\n",
    "feat_sel = fs.VarianceThreshold(threshold=(0.1))\n",
    "var_sel = feat_sel.fit_transform(X)\n",
    "\n",
    "# from X, drop the columns for the features whose variance did not meet the threshold, and \n",
    "# get dummy variables for remaining categorical features.\n",
    "X2 = pd.get_dummies(X.drop(X.columns[~feat_sel.get_support()], axis=1)).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit linear models on X (original with dummy variables for categorical) and X2 (with feature selection for V[x] > 0.1)\n",
    "reg = lm.LinearRegression().fit(X, y)\n",
    "reg\n",
    "\n",
    "reg2 = lm.LinearRegression().fit(X2, y)\n",
    "reg2\n",
    "\n",
    "# Ridge Regression | L2 norm aka Euclidean norm aka the sqrt(sum(abs(Beta)^2)). Regularizes\n",
    "# the loss with lambda*sum(Beta^2)\n",
    "# NOTE: Increasing tolerance to 10, and max_iter to 10^6 did not bring the objective function to converge\n",
    "reg3 = lm.Ridge(alpha=0.5, tol=10, max_iter=1000000).fit(X, y)\n",
    "reg3\n",
    "\n",
    "# Lasso Regression | L1 norm aka sum(abs(Beta)). Regularizes the loss with lambda*sum(abs(Beta))\n",
    "reg4 = lm.Lasso(alpha=1.0).fit(X, y)\n",
    "reg4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('y_hat_X')\n",
    "reg.score(X, y)\n",
    "reg.intercept_\n",
    "\n",
    "print('y_hat_X2')\n",
    "reg2.score(X2, y)\n",
    "reg2.intercept_\n",
    "\n",
    "# ridge with lambda = 1.0\n",
    "print('y_hat_ridge')\n",
    "reg3.score(X, y)\n",
    "reg3.intercept_\n",
    "\n",
    "# lasso with lambda = 1.0\n",
    "print('y_hat_lasso')\n",
    "reg4.score(X, y)\n",
    "reg4.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shows that as lambda increases, the R^2 value decreases\n",
    "# Also, the regular linear model had better performance, with R^2 of 0.9454\n",
    "print('Running Ridge for lambda in [0.00, 0.50]')\n",
    "for i in np.arange(0.00, 0.50, 0.05):\n",
    "    a = round(i, 3)\n",
    "    reg6 = lm.Ridge(alpha=a).fit(X, y)\n",
    "    print(a)\n",
    "    print(reg6.score(X, y))\n",
    "    print('\\n')\n",
    "\n",
    "# Running a Lasso returns a max of about 0.9454198...\n",
    "print('Running Lasso for lambda in [0.050, 0.50]')\n",
    "for i in np.arange(0.050, 0.50, 0.05):\n",
    "    a = round(i, 3)\n",
    "    reg7 = lm.Lasso(alpha=a).fit(X, y)\n",
    "    print(a)\n",
    "    print(reg7.score(X, y))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the tests above, the $R^2$ to beat is 0.9454. I will continue first with testing feature selection methods. If I can achieve a sufficient improvement in $R^2$ with a subset of features, I will proceed to advanced regression techniques with that only those features. Otherwise, I will continue to performing advanced regressions with the full feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Elasticnet: uses both the Ridge and Lasso penalties. Drawback of Lasso is that if there are a number of correlated \n",
    "# variables, it will only select one, and deselect for the rest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "{\\hat {\\beta }}\\equiv {\\underset {\\beta }{\\operatorname {argmin} }}(\\|y-X\\beta \\|^{2}+\\lambda _{2}\\|\\beta \\|^2_2+\\lambda _{1}\\|\\beta \\|_{1})\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn's elasticnet implementation allows you to attribute weights (which sum to 1) to each of the penalties. Setting $l1_{ratio} = 1 $ will apply the Lasso, or $L1$ norm, penalty and setting $l1_{ratio} = 0$ will apply the Ridge, or $L2$ norm penalty. When $0 < l1_{ratio} < 1$ will apply some fraction of both penalties. The following is the objective function the implementation aims to minimize\n",
    "\n",
    "\\begin{equation*}\n",
    "{\\frac{1}{2*n_{samples}} * ||y - X\\beta||^2_2\n",
    "+ \\alpha * l1_{ratio} * ||\\beta||_1\n",
    "+ 0.5 * \\alpha * (1 - l1_{ratio}) * ||\\beta||^2_2}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elas_net = lm.ElasticNet(alpha=0.00001, \n",
    "                         random_state=0, \n",
    "                         l1_ratio=0.01, \n",
    "                         max_iter=100000, \n",
    "                         tol=0.001)\\\n",
    "                    .fit(X, y)\n",
    "elas_net.score(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elas_net.dual_gap_\n",
    "elas_net.tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
